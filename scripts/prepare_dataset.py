"""
æ•°æ®é›†å‡†å¤‡è„šæœ¬
ç”¨äºå¤„ç†å’Œå‡†å¤‡æ–°æ•°æ®é›†
"""

import pandas as pd
import numpy as np
import scipy.sparse as sp
import pickle as pkl
import os
import sys
from collections import defaultdict

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

class NewDatasetPreparer:
    def __init__(self, raw_data_dir='./raw_data', 
                 output_dir='./processed_data'):
        """
        å‡†å¤‡æ–°æ•°æ®é›†
        
        Args:
            raw_data_dir: åŸå§‹æ•°æ®ç›®å½•ï¼ˆåŒ…å«CSVæ–‡ä»¶ï¼‰
            output_dir: å¤„ç†åçš„æ•°æ®è¾“å‡ºç›®å½•
        """
        self.raw_data_dir = raw_data_dir
        self.output_dir = output_dir
        self.user_map = {}
        self.item_map = {}
        
    def load_and_process(self):
        """åŠ è½½å¹¶å¤„ç†æ•°æ®"""
        print("="*60)
        print("ACKRec æ•°æ®é›†å‡†å¤‡å·¥å…·")
        print("="*60)
        
        # 1. ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
        # 2. åŠ è½½äº¤äº’æ•°æ®
        interactions_path = os.path.join(self.raw_data_dir, 'interactions.csv')
        if not os.path.exists(interactions_path):
            print(f"âš ï¸ æ‰¾ä¸åˆ°äº¤äº’æ–‡ä»¶: {interactions_path}")
            print("è¯·ç¡®ä¿æœ‰ä»¥ä¸‹æ–‡ä»¶åœ¨ raw_data/ ç›®å½•:")
            print("  - interactions.csv (å¿…éœ€)")
            print("  - users.csv (å¯é€‰)")
            print("  - items.csv (å¯é€‰)")
            return None
        
        print(f"ğŸ“Š åŠ è½½äº¤äº’æ•°æ®: {interactions_path}")
        try:
            interactions = pd.read_csv(interactions_path)
        except Exception as e:
            print(f"âŒ åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {e}")
            return None
        
        # 3. åˆ›å»ºIDæ˜ å°„ï¼ˆå°†åŸå§‹IDæ˜ å°„åˆ°è¿ç»­ç´¢å¼•ï¼‰
        unique_users = sorted(interactions['user_id'].unique())
        unique_items = sorted(interactions['item_id'].unique())
        
        self.user_map = {uid: idx for idx, uid in enumerate(unique_users)}
        self.item_map = {iid: idx for idx, iid in enumerate(unique_items)}
        
        self.num_users = len(unique_users)
        self.num_items = len(unique_items)
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"   ç”¨æˆ·æ•°é‡: {self.num_users}")
        print(f"   ç‰©å“æ•°é‡: {self.num_items}")
        print(f"   äº¤äº’æ•°é‡: {len(interactions)}")
        
        # 4. åˆ›å»ºè¯„åˆ†çŸ©é˜µ
        print("\nğŸ“ˆ åˆ›å»ºè¯„åˆ†çŸ©é˜µ...")
        rows = interactions['user_id'].map(self.user_map)
        cols = interactions['item_id'].map(self.item_map)
        
        # å¦‚æœæœ‰è¯„åˆ†åˆ—å°±ä½¿ç”¨ï¼Œå¦åˆ™ç”¨1è¡¨ç¤ºäº¤äº’
        if 'rating' in interactions.columns:
            values = interactions['rating'].values
            print(f"   ä½¿ç”¨è¯„åˆ†åˆ—: rating")
        else:
            values = np.ones(len(interactions))
            print(f"   æ— è¯„åˆ†åˆ—ï¼Œä½¿ç”¨é»˜è®¤å€¼: 1")
        
        rating_matrix = sp.csr_matrix(
            (values, (rows, cols)), 
            shape=(self.num_users, self.num_items)
        )
        
        density = rating_matrix.nnz / (self.num_users * self.num_items) * 100
        print(f"   è¯„åˆ†çŸ©é˜µå¯†åº¦: {density:.4f}%")
        
        # 5. ç”Ÿæˆè´Ÿæ ·æœ¬
        print("\nğŸ¯ ç”Ÿæˆè´Ÿæ ·æœ¬...")
        negative_samples = self.generate_negative_samples(interactions)
        
        # 6. åˆ›å»ºç‰¹å¾
        print("\nğŸ”§ åˆ›å»ºç‰¹å¾...")
        user_features = self.create_user_features()
        item_features = self.create_item_features()
        
        # 7. åˆ›å»ºé‚»æ¥çŸ©é˜µ
        print("\nğŸ”— åˆ›å»ºé‚»æ¥çŸ©é˜µ...")
        adjacency_matrices = self.create_adjacency_matrices(rating_matrix)
        
        # 8. ä¿å­˜æ‰€æœ‰æ•°æ®
        print("\nğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
        self.save_all_data(
            rating_matrix=rating_matrix,
            negative_samples=negative_samples,
            user_features=user_features,
            item_features=item_features,
            adjacency_matrices=adjacency_matrices
        )
        
        # 9. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
        print("="*60)
        print(f"ç”¨æˆ·æ•°é‡: {self.num_users}")
        print(f"ç‰©å“æ•°é‡: {self.num_items}")
        print(f"äº¤äº’æ•°é‡: {len(interactions)}")
        print(f"è¯„åˆ†çŸ©é˜µå½¢çŠ¶: {rating_matrix.shape}")
        print(f"è¯„åˆ†å¯†åº¦: {density:.4f}%")
        print(f"ç”¨æˆ·ç‰¹å¾å½¢çŠ¶: {user_features.shape}")
        print(f"ç‰©å“ç‰¹å¾å½¢çŠ¶: {item_features.shape}")
        print(f"è´Ÿæ ·æœ¬å½¢çŠ¶: {negative_samples.shape}")
        print(f"é‚»æ¥çŸ©é˜µ: {list(adjacency_matrices.keys())}")
        print("="*60)
        print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆ!")
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {self.output_dir}")
        
        return {
            'num_users': self.num_users,
            'num_items': self.num_items,
            'rating_shape': rating_matrix.shape,
            'density': density
        }
    
    def generate_negative_samples(self, interactions, num_negatives=99):
        """
        ç”Ÿæˆè´Ÿæ ·æœ¬
        æ ¼å¼ï¼šæ¯ä¸ªç”¨æˆ·100ä¸ªæ ·æœ¬ï¼ˆ99è´Ÿ + 1æ­£ï¼‰
        """
        print(f"   æ¯ä¸ªç”¨æˆ·ç”Ÿæˆ {num_negatives} ä¸ªè´Ÿæ ·æœ¬ + 1 ä¸ªæ­£æ ·æœ¬")
        
        # è·å–æ¯ä¸ªç”¨æˆ·çš„äº¤äº’ç‰©å“
        user_interactions = defaultdict(set)
        for _, row in interactions.iterrows():
            user_idx = self.user_map[row['user_id']]
            item_idx = self.item_map[row['item_id']]
            user_interactions[user_idx].add(item_idx)
        
        all_items = list(range(self.num_items))
        negative_samples = []
        
        valid_users = 0
        for user_idx in range(self.num_users):
            # è·å–è¯¥ç”¨æˆ·äº¤äº’è¿‡çš„ç‰©å“
            positive_items = user_interactions.get(user_idx, set())
            
            if not positive_items:
                # å¦‚æœç”¨æˆ·æ²¡æœ‰äº¤äº’ï¼Œè·³è¿‡
                continue
            
            valid_users += 1
            
            # ç”Ÿæˆè´Ÿæ ·æœ¬
            negative_candidates = []
            attempts = 0
            max_attempts = num_negatives * 10
            
            while len(negative_candidates) < num_negatives and attempts < max_attempts:
                candidate = np.random.choice(all_items)
                if candidate not in positive_items and candidate not in negative_candidates:
                    negative_candidates.append(candidate)
                attempts += 1
            
            # å¦‚æœè´Ÿæ ·æœ¬ä¸å¤Ÿï¼Œç”¨éšæœºç‰©å“å¡«å……ï¼ˆå…è®¸é‡å¤ï¼‰
            while len(negative_candidates) < num_negatives:
                candidate = np.random.choice(all_items)
                negative_candidates.append(candidate)
            
            # é€‰æ‹©ä¸€ä¸ªæ­£æ ·æœ¬
            positive_sample = np.random.choice(list(positive_items))
            
            # æ„å»ºæ ·æœ¬ï¼š[ç”¨æˆ·ID, ç‰©å“ID]
            samples = [[user_idx, item] for item in negative_candidates]
            samples.append([user_idx, positive_sample])  # æ­£æ ·æœ¬åœ¨æœ€å
            
            negative_samples.append(samples)
        
        print(f"   æœ‰æ•ˆç”¨æˆ·æ•°: {valid_users}/{self.num_users}")
        
        return np.array(negative_samples, dtype=np.int32)
    
    def create_user_features(self):
        """åˆ›å»ºç”¨æˆ·ç‰¹å¾"""
        # å¦‚æœæœ‰ç”¨æˆ·ç‰¹å¾æ–‡ä»¶å°±åŠ è½½
        user_features_path = os.path.join(self.raw_data_dir, 'users.csv')
        
        if os.path.exists(user_features_path):
            print(f"   åŠ è½½ç”¨æˆ·ç‰¹å¾: {user_features_path}")
            try:
                users_df = pd.read_csv(user_features_path)
                
                # è¿‡æ»¤å¹¶æ’åº
                users_df = users_df[users_df['user_id'].isin(self.user_map.keys())]
                users_df['mapped_id'] = users_df['user_id'].map(self.user_map)
                users_df = users_df.sort_values('mapped_id')
                
                # æå–ç‰¹å¾åˆ—ï¼ˆæ’é™¤IDåˆ—ï¼‰
                feature_cols = [col for col in users_df.columns 
                              if col not in ['user_id', 'mapped_id']]
                
                if len(feature_cols) > 0:
                    features = users_df[feature_cols].values
                    
                    # å½’ä¸€åŒ–
                    features = features.astype(np.float32)
                    row_sums = features.sum(axis=1)
                    row_sums[row_sums == 0] = 1  # é¿å…é™¤ä»¥0
                    features = features / row_sums[:, np.newaxis]
                    
                    print(f"   ç”¨æˆ·ç‰¹å¾ç»´åº¦: {features.shape}")
                    return features
                else:
                    print("   âš ï¸ ç”¨æˆ·ç‰¹å¾æ–‡ä»¶æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾åˆ—")
            except Exception as e:
                print(f"   âš ï¸ åŠ è½½ç”¨æˆ·ç‰¹å¾å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ç‰¹å¾æ–‡ä»¶ï¼Œä½¿ç”¨one-hotç¼–ç 
        print("   ä½¿ç”¨å•ä½çŸ©é˜µä½œä¸ºç”¨æˆ·ç‰¹å¾")
        features = np.eye(self.num_users, dtype=np.float32)
        print(f"   ç”¨æˆ·ç‰¹å¾ç»´åº¦: {features.shape}")
        return features
    
    def create_item_features(self):
        """åˆ›å»ºç‰©å“ç‰¹å¾"""
        # ç±»ä¼¼ç”¨æˆ·ç‰¹å¾çš„å¤„ç†
        item_features_path = os.path.join(self.raw_data_dir, 'items.csv')
        
        if os.path.exists(item_features_path):
            print(f"   åŠ è½½ç‰©å“ç‰¹å¾: {item_features_path}")
            try:
                items_df = pd.read_csv(item_features_path)
                
                # è¿‡æ»¤å¹¶æ’åº
                items_df = items_df[items_df['item_id'].isin(self.item_map.keys())]
                items_df['mapped_id'] = items_df['item_id'].map(self.item_map)
                items_df = items_df.sort_values('mapped_id')
                
                # æå–ç‰¹å¾åˆ—
                feature_cols = [col for col in items_df.columns 
                              if col not in ['item_id', 'mapped_id']]
                
                if len(feature_cols) > 0:
                    features = items_df[feature_cols].values
                    
                    # å½’ä¸€åŒ–
                    features = features.astype(np.float32)
                    row_sums = features.sum(axis=1)
                    row_sums[row_sums == 0] = 1
                    features = features / row_sums[:, np.newaxis]
                    
                    print(f"   ç‰©å“ç‰¹å¾ç»´åº¦: {features.shape}")
                    return features
                else:
                    print("   âš ï¸ ç‰©å“ç‰¹å¾æ–‡ä»¶æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾åˆ—")
            except Exception as e:
                print(f"   âš ï¸ åŠ è½½ç‰©å“ç‰¹å¾å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ç‰¹å¾æ–‡ä»¶ï¼Œä½¿ç”¨one-hotç¼–ç 
        print("   ä½¿ç”¨å•ä½çŸ©é˜µä½œä¸ºç‰©å“ç‰¹å¾")
        features = np.eye(self.num_items, dtype=np.float32)
        print(f"   ç‰©å“ç‰¹å¾ç»´åº¦: {features.shape}")
        return features
    
    def create_adjacency_matrices(self, rating_matrix):
        """åˆ›å»ºå„ç§é‚»æ¥çŸ©é˜µ"""
        print("   åˆ›å»ºé‚»æ¥çŸ©é˜µ...")
        adjacency_matrices = {}
        
        try:
            # 1. UKçŸ©é˜µ (ç”¨æˆ·-ç‰©å“)
            uk_matrix = rating_matrix.copy()
            
            # 2. UKUçŸ©é˜µ (ç”¨æˆ·-ç‰©å“-ç”¨æˆ·)
            print("     - UKUçŸ©é˜µ (ç”¨æˆ·-ç‰©å“-ç”¨æˆ·)")
            uku_matrix = uk_matrix.dot(uk_matrix.T)
            # æ·»åŠ è‡ªè¿æ¥å¹¶å½’ä¸€åŒ–
            uku_matrix = self.normalize_adjacency(uku_matrix)
            adjacency_matrices['uku'] = uku_matrix
            
            # 3. KUKçŸ©é˜µ (ç‰©å“-ç”¨æˆ·-ç‰©å“)
            print("     - KUKçŸ©é˜µ (ç‰©å“-ç”¨æˆ·-ç‰©å“)")
            kuk_matrix = uk_matrix.T.dot(uk_matrix)
            kuk_matrix = self.normalize_adjacency(kuk_matrix)
            adjacency_matrices['kuk'] = kuk_matrix
            
            # 4. UCUçŸ©é˜µ (ç”¨æˆ·ç‰¹å¾ç›¸ä¼¼åº¦)
            print("     - UCUçŸ©é˜µ (ç”¨æˆ·ç‰¹å¾ç›¸ä¼¼åº¦)")
            try:
                with open(os.path.join(self.output_dir, 'UC.p'), 'rb') as f:
                    uc = pkl.load(f)
                    if hasattr(uc, 'todense'):
                        uc = uc.todense()
                uc = uc.dot(uc.T) + np.eye(uc.shape[0])
                ucu = self.normalize_adjacency(uc)
                adjacency_matrices['ucu'] = ucu
            except:
                print("       âš ï¸ æ— æ³•åˆ›å»ºUCUçŸ©é˜µï¼Œä½¿ç”¨UKUæ›¿ä»£")
                adjacency_matrices['ucu'] = uku_matrix
            
            print(f"   æˆåŠŸåˆ›å»º {len(adjacency_matrices)} ä¸ªé‚»æ¥çŸ©é˜µ")
            
        except Exception as e:
            print(f"   âš ï¸ åˆ›å»ºé‚»æ¥çŸ©é˜µæ—¶å‡ºé”™: {e}")
            # åˆ›å»ºç®€å•çš„å•ä½çŸ©é˜µä½œä¸ºå¤‡ç”¨
            identity_user = np.eye(self.num_users)
            identity_item = np.eye(self.num_items)
            adjacency_matrices['uku'] = identity_user
            adjacency_matrices['kuk'] = identity_item
        
        return adjacency_matrices
    
    def normalize_adjacency(self, adjacency):
        """å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ"""
        if sp.issparse(adjacency):
            adjacency = adjacency.toarray()
        
        # æ·»åŠ è‡ªè¿æ¥
        adjacency = adjacency + np.eye(adjacency.shape[0])
        
        # å¯¹ç§°å½’ä¸€åŒ–: D^(-1/2) * A * D^(-1/2)
        rowsum = np.array(adjacency.sum(1))
        d_inv_sqrt = np.power(rowsum, -0.5).flatten()
        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
        d_mat_inv_sqrt = np.diag(d_inv_sqrt)
        
        normalized = d_mat_inv_sqrt.dot(adjacency).dot(d_mat_inv_sqrt)
        
        # ç¼©æ”¾
        normalized = normalized * 100
        
        return normalized
    
    def save_all_data(self, rating_matrix, negative_samples, 
                     user_features, item_features, adjacency_matrices):
        """ä¿å­˜æ‰€æœ‰æ•°æ®"""
        try:
            # 1. ä¿å­˜è¯„åˆ†çŸ©é˜µ
            pkl.dump(
                rating_matrix,
                open(os.path.join(self.output_dir, 'rate_matrix.p'), 'wb')
            )
            print("   âœ… ä¿å­˜è¯„åˆ†çŸ©é˜µ")
            
            # 2. ä¿å­˜è´Ÿæ ·æœ¬
            pkl.dump(
                negative_samples,
                open(os.path.join(self.output_dir, 'negative.p'), 'wb')
            )
            print("   âœ… ä¿å­˜è´Ÿæ ·æœ¬")
            
            # 3. ä¿å­˜ç”¨æˆ·ç‰¹å¾
            pkl.dump(
                user_features,
                open(os.path.join(self.output_dir, 'UC.p'), 'wb')
            )
            print("   âœ… ä¿å­˜ç”¨æˆ·ç‰¹å¾")
            
            # 4. ä¿å­˜ç‰©å“ç‰¹å¾
            pkl.dump(
                item_features,
                open(os.path.join(self.output_dir, 'concept_feature_bow.p'), 'wb')
            )
            print("   âœ… ä¿å­˜ç‰©å“ç‰¹å¾")
            
            # 5. ä¿å­˜åµŒå…¥ç‰¹å¾ï¼ˆå¯ä»¥ç”¨éšæœºåˆå§‹åŒ–ï¼‰
            embedding_dim = min(50, item_features.shape[1])
            if item_features.shape[1] > embedding_dim:
                # ä½¿ç”¨PCAé™ç»´
                try:
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=embedding_dim)
                    concept_embedding = pca.fit_transform(item_features)
                    print(f"   âœ… ä½¿ç”¨PCAé™ç»´åˆ° {embedding_dim} ç»´")
                except:
                    concept_embedding = item_features[:, :embedding_dim]
                    print(f"   âœ… æˆªå–å‰ {embedding_dim} ç»´ç‰¹å¾")
            else:
                concept_embedding = item_features
            
            pkl.dump(
                concept_embedding,
                open(os.path.join(self.output_dir, 'concept_embedding.p'), 'wb')
            )
            print("   âœ… ä¿å­˜åµŒå…¥ç‰¹å¾")
            
            # 6. ä¿å­˜é‚»æ¥çŸ©é˜µ
            for name, matrix in adjacency_matrices.items():
                pkl.dump(
                    matrix,
                    open(os.path.join(self.output_dir, f'{name}_matrix.p'), 'wb')
                )
                print(f"   âœ… ä¿å­˜ {name} çŸ©é˜µ")
            
            # 7. ä¿å­˜ä¸»è¦é‚»æ¥çŸ©é˜µï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
            if 'uku' in adjacency_matrices:
                pkl.dump(
                    adjacency_matrices['uku'],
                    open(os.path.join(self.output_dir, 'adjacency_matrix.p'), 'wb')
                )
                print("   âœ… ä¿å­˜ä¸»è¦é‚»æ¥çŸ©é˜µ")
            
            # 8. ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'num_users': self.num_users,
                'num_items': self.num_items,
                'user_map': self.user_map,
                'item_map': self.item_map,
                'created_at': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            pkl.dump(
                metadata,
                open(os.path.join(self.output_dir, 'metadata.p'), 'wb')
            )
            print("   âœ… ä¿å­˜å…ƒæ•°æ®")
            
            # 9. ä¿å­˜æ–‡æœ¬æ ¼å¼çš„å…ƒæ•°æ®ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
            with open(os.path.join(self.output_dir, 'dataset_info.txt'), 'w') as f:
                f.write("="*60 + "\n")
                f.write("ACKRec æ•°æ®é›†ä¿¡æ¯\n")
                f.write("="*60 + "\n\n")
                f.write(f"ç”¨æˆ·æ•°é‡: {self.num_users}\n")
                f.write(f"ç‰©å“æ•°é‡: {self.num_items}\n")
                f.write(f"è¯„åˆ†çŸ©é˜µå½¢çŠ¶: {rating_matrix.shape}\n")
                f.write(f"è¯„åˆ†å¯†åº¦: {rating_matrix.nnz / (self.num_users * self.num_items):.4f}\n")
                f.write(f"è´Ÿæ ·æœ¬å½¢çŠ¶: {negative_samples.shape}\n")
                f.write(f"ç”¨æˆ·ç‰¹å¾å½¢çŠ¶: {user_features.shape}\n")
                f.write(f"ç‰©å“ç‰¹å¾å½¢çŠ¶: {item_features.shape}\n")
                f.write(f"é‚»æ¥çŸ©é˜µ: {list(adjacency_matrices.keys())}\n")
                f.write(f"\nç”Ÿæˆæ—¶é—´: {metadata['created_at']}\n")
            
            print("   âœ… ä¿å­˜æ•°æ®é›†ä¿¡æ¯")
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    print("ACKRec æ•°æ®é›†å‡†å¤‡å·¥å…·")
    print("-" * 40)
    
    # æ£€æŸ¥åŸå§‹æ•°æ®ç›®å½•
    raw_dir = './raw_data'
    if not os.path.exists(raw_dir):
        print(f"åŸå§‹æ•°æ®ç›®å½• '{raw_dir}' ä¸å­˜åœ¨")
        print("åˆ›å»ºç¤ºä¾‹ç›®å½•ç»“æ„...")
        os.makedirs(raw_dir, exist_ok=True)
        
        # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
        example_data = {
            'user_id': [1, 1, 2, 2, 3],
            'item_id': [101, 102, 101, 103, 102],
            'rating': [5, 3, 4, 2, 5]
        }
        example_df = pd.DataFrame(example_data)
        example_df.to_csv(os.path.join(raw_dir, 'interactions.csv'), index=False)
        
        print(f"å·²åœ¨ '{raw_dir}' åˆ›å»ºç¤ºä¾‹ interactions.csv æ–‡ä»¶")
        print("è¯·å°†æ‚¨çš„æ•°æ®æ–‡ä»¶æ”¾å…¥è¯¥ç›®å½•åé‡æ–°è¿è¡Œ")
        return
    
    # åˆ›å»ºå‡†å¤‡å™¨
    preparer = NewDatasetPreparer(
        raw_data_dir=raw_dir,
        output_dir='./processed_data'
    )
    
    # å¤„ç†æ•°æ®
    try:
        stats = preparer.load_and_process()
        if stats:
            print(f"\nğŸ‰ æ•°æ®é›†å‡†å¤‡å®Œæˆ!")
            print(f"å¤„ç†åçš„æ•°æ®ä¿å­˜åœ¨: ./processed_data/")
    except Exception as e:
        print(f"\nâŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main())