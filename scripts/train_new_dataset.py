"""
æ–°æ•°æ®é›†è®­ç»ƒè„šæœ¬
ä½¿ç”¨è‡ªåŠ¨é…ç½®è®­ç»ƒACKRecæ¨¡å‹
"""

import numpy as np
import torch
import random
import time
import os
import sys
import pickle as pkl
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from models.models import AGCNrec
from utils.data_utils import load_new_dataset
from config import ExperimentConfig, DatasetConfig, TrainingConfig
from utils.metrics import print_metrics

class NewDatasetTrainer:
    """æ–°æ•°æ®é›†çš„è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: å®Œæ•´çš„é…ç½®å­—å…¸
        """
        self.config = config
        self.model = None
        self.batch_data = None
        
        # è®¾ç½®éšæœºç§å­
        self.set_random_seed()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.create_output_dir()
        
    def set_random_seed(self):
        """è®¾ç½®éšæœºç§å­"""
        seed = self.config['training']['seed']
        np.random.seed(seed)
        torch.manual_seed(seed)
        random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
            
        print(f"âœ… éšæœºç§å­è®¾ç½®ä¸º: {seed}")
    
    def create_output_dir(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        output_dir = self.config['training']['output_dir']
        
        # æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.output_dir = os.path.join(output_dir, f'run_{timestamp}')
        
        os.makedirs(self.output_dir, exist_ok=True)
        print(f"âœ… è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # ä¿å­˜é…ç½®
        config_file = os.path.join(self.output_dir, 'config.pkl')
        with open(config_file, 'wb') as f:
            pkl.dump(self.config, f)
        
        # ä¿å­˜æ–‡æœ¬æ ¼å¼çš„é…ç½®
        config_text = os.path.join(self.output_dir, 'config.txt')
        with open(config_text, 'w') as f:
            f.write("="*60 + "\n")
            f.write("ACKRec å®éªŒé…ç½®\n")
            f.write("="*60 + "\n\n")
            
            f.write("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:\n")
            stats = self.config['stats']
            for key, value in stats.items():
                f.write(f"  {key}: {value}\n")
            
            f.write("\nğŸ¤– æ¨¡å‹é…ç½®:\n")
            model_config = self.config['model']
            for key, value in model_config.items():
                if key != 'description':
                    f.write(f"  {key}: {value}\n")
            f.write(f"  æè¿°: {model_config['description']}\n")
            
            f.write("\nâš™ï¸ è®­ç»ƒé…ç½®:\n")
            train_config = self.config['training']
            for key, value in train_config.items():
                f.write(f"  {key}: {value}\n")
        
        print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {config_text}")
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("\n" + "="*50)
        print("ğŸ“‚ åŠ è½½æ•°æ®")
        print("="*50)
        
        dataset_config = self.config['dataset']
        
        try:
            rating, features_item, features_user, support_user, support_item, negative = load_new_dataset(
                data_dir=dataset_config['data_dir']
            )
            
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
            print(f"   è¯„åˆ†çŸ©é˜µå½¢çŠ¶: {rating.shape}")
            print(f"   ç”¨æˆ·ç‰¹å¾å½¢çŠ¶: {features_user.shape}")
            print(f"   ç‰©å“ç‰¹å¾å½¢çŠ¶: {features_item.shape}")
            print(f"   è´Ÿæ ·æœ¬å½¢çŠ¶: {negative.shape}")
            print(f"   ç”¨æˆ·æ”¯æŒçŸ©é˜µæ•°é‡: {len(support_user)}")
            print(f"   ç‰©å“æ”¯æŒçŸ©é˜µæ•°é‡: {len(support_item)}")
            
            # ä¿å­˜æ•°æ®ä¿¡æ¯
            self.dataset_info = {
                'user_dim': rating.shape[0],
                'item_dim': rating.shape[1],
                'input_dim_user': features_user.shape[1],
                'input_dim_item': features_item.shape[1]
            }
            
            return rating, features_item, features_user, support_user, support_item, negative
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            print("å°è¯•ä½¿ç”¨è™šæ‹Ÿæ•°æ®...")
            from utils.data_utils import create_dummy_data
            return create_dummy_data(
                num_users=self.config['stats'].get('num_users', 100),
                num_items=self.config['stats'].get('num_items', 50)
            )
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        print("\n" + "="*50)
        print("ğŸ¤– åˆ›å»ºæ¨¡å‹")
        print("="*50)
        
        # ä»é…ç½®è·å–å‚æ•°
        model_config = self.config['model']
        
        # åˆ›å»ºplaceholders
        placeholders = {
            'rating': self.rating,
            'features_user': self.features_user,
            'features_item': self.features_item,
            'negative': self.negative
        }
        
        # åˆ›å»ºæ¨¡å‹
        print(f"ä½¿ç”¨æ¨¡å‹é…ç½®:")
        print(f"  éšè—å±‚ç»´åº¦: {model_config['hidden_dims']}")
        print(f"  è¾“å‡ºç»´åº¦: {model_config['output_dim']}")
        print(f"  æ½œåœ¨ç»´åº¦: {model_config['latent_dim']}")
        print(f"  æ³¨æ„åŠ›å¤§å°: {model_config['attention_size']}")
        print(f"  Dropoutç‡: {model_config['dropout_rate']}")
        print(f"  å­¦ä¹ ç‡: {model_config['learning_rate']}")
        print(f"  è®­ç»ƒè½®æ•°: {model_config['epochs']}")
        
        self.model = AGCNrec(
            placeholders=placeholders,
            input_dim_user=self.dataset_info['input_dim_user'],
            input_dim_item=self.dataset_info['input_dim_item'],
            user_dim=self.dataset_info['user_dim'],
            item_dim=self.dataset_info['item_dim'],
            learning_rate=model_config['learning_rate']
        )
        
        # è®¾ç½®è®¾å¤‡
        device = self.config['training']['device']
        self.model = self.model.to(device)
        print(f"âœ… æ¨¡å‹å·²åˆ›å»ºå¹¶ç§»åŠ¨åˆ°: {device}")
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"âœ… æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        
        # ä¿å­˜æ¨¡å‹ç»“æ„
        model_structure_file = os.path.join(self.output_dir, 'model_structure.txt')
        with open(model_structure_file, 'w') as f:
            f.write(str(self.model))
        print(f"âœ… æ¨¡å‹ç»“æ„å·²ä¿å­˜åˆ°: {model_structure_file}")
    
    def prepare_batch_data(self):
        """å‡†å¤‡æ‰¹å¤„ç†æ•°æ®"""
        device = self.config['training']['device']
        
        self.batch_data = {
            'features_user': self.features_user.to(device),
            'features_item': self.features_item.to(device),
            'rating': self.rating.to(device),
            'supports_user': [sup.to(device) for sup in self.support_user],
            'supports_item': [sup.to(device) for sup in self.support_item],
            'negative': self.negative.to(device)
        }
        
        print(f"âœ… æ‰¹å¤„ç†æ•°æ®å·²å‡†å¤‡")
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        print("\n" + "="*50)
        print("ğŸš€ å¼€å§‹è®­ç»ƒ")
        print("="*50)
        
        # è·å–è®­ç»ƒå‚æ•°
        epochs = self.config['model']['epochs']
        eval_frequency = self.config['training']['eval_frequency']
        output_dir = self.output_dir
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        log_file = os.path.join(output_dir, 'training_log.csv')
        with open(log_file, 'w') as f:
            f.write('epoch,loss,hr1,hr5,hr10,hr20,ndcg5,ndcg10,ndcg20,mrr,auc,time\n')
        
        print(f"è®­ç»ƒå‚æ•°:")
        print(f"  æ€»è½®æ•°: {epochs}")
        print(f"  è¯„ä¼°é¢‘ç‡: æ¯ {eval_frequency} è½®")
        print(f"  æ—¥å¿—æ–‡ä»¶: {log_file}")
        print("-" * 80)
        print(f"{'è½®æ¬¡':^6} | {'æŸå¤±':^10} | {'HR@10':^8} | {'NDCG@10':^8} | {'MRR':^8} | {'AUC':^8} | {'æ—¶é—´':^6}")
        print("-" * 80)
        
        # è®­ç»ƒå¾ªç¯
        best_hr10 = 0.0
        best_epoch = 0
        start_time = time.time()
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒæ­¥éª¤
            loss = self.model.train_step(self.batch_data)
            epoch_time = time.time() - epoch_start
            
            # è¯„ä¼°
            if epoch % eval_frequency == 0 or epoch == epochs - 1:
                metrics = self.model.evaluate(self.batch_data)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if metrics['hr@10'] > best_hr10:
                    best_hr10 = metrics['hr@10']
                    best_epoch = epoch
                    model_path = os.path.join(output_dir, f'best_model_epoch{epoch}.pth')
                    self.model.save(model_path)
                
                # æ‰“å°è¿›åº¦
                print(f"{epoch:6d} | {loss:10.4f} | {metrics['hr@10']:8.4f} | "
                      f"{metrics['ndcg@10']:8.4f} | {metrics['mrr']:8.4f} | "
                      f"{metrics['auc']:8.4f} | {epoch_time:6.1f}s")
                
                # è®°å½•æ—¥å¿—
                with open(log_file, 'a') as f:
                    f.write(f"{epoch},{loss:.4f},"
                           f"{metrics['hr@1']:.4f},{metrics['hr@5']:.4f},"
                           f"{metrics['hr@10']:.4f},{metrics['hr@20']:.4f},"
                           f"{metrics['ndcg@5']:.4f},{metrics['ndcg@10']:.4f},"
                           f"{metrics['ndcg@20']:.4f},{metrics['mrr']:.4f},"
                           f"{metrics['auc']:.4f},{epoch_time:.1f}\n")
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print("-" * 80)
        print(f"âœ… è®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time:.1f}ç§’")
        print(f"ğŸ¯ æœ€ä½³HR@10: {best_hr10:.4f} (epoch {best_epoch})")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(output_dir, 'final_model.pth')
        self.model.save(final_model_path)
        print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
        print("\nğŸ“Š åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
        best_model_path = os.path.join(output_dir, f'best_model_epoch{best_epoch}.pth')
        self.model.load(best_model_path)
        final_metrics = self.model.evaluate(self.batch_data)
        
        print("\nğŸ“ˆ æœ€ç»ˆè¯„ä¼°ç»“æœ:")
        print_metrics(final_metrics, prefix="  ")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results(best_epoch, best_hr10, total_time, final_metrics)
        
        return final_metrics
    
    def save_final_results(self, best_epoch, best_hr10, total_time, metrics):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        result_file = os.path.join(self.output_dir, 'final_results.txt')
        
        with open(result_file, 'w') as f:
            f.write("="*60 + "\n")
            f.write("ğŸ‰ ACKRec è®­ç»ƒæœ€ç»ˆç»“æœ\n")
            f.write("="*60 + "\n\n")
            
            f.write("ğŸ“‹ è®­ç»ƒæ‘˜è¦:\n")
            f.write(f"  æœ€ä½³è½®æ¬¡: {best_epoch}\n")
            f.write(f"  æœ€ä½³HR@10: {best_hr10:.4f}\n")
            f.write(f"  æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’\n\n")
            
            f.write("ğŸ“Š æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡:\n")
            f.write("-" * 40 + "\n")
            
            # åˆ†ç»„æ˜¾ç¤ºæŒ‡æ ‡
            hr_metrics = {k: v for k, v in metrics.items() if k.startswith('hr@')}
            ndcg_metrics = {k: v for k, v in metrics.items() if k.startswith('ndcg@')}
            other_metrics = {k: v for k, v in metrics.items() if not k.startswith('hr@') and not k.startswith('ndcg@')}
            
            if hr_metrics:
                f.write("Hit Rate (HR):\n")
                for k in sorted(hr_metrics.keys(), key=lambda x: int(x.split('@')[1])):
                    f.write(f"  {k:8}: {hr_metrics[k]:.4f}\n")
                f.write("\n")
            
            if ndcg_metrics:
                f.write("Normalized DCG:\n")
                for k in sorted(ndcg_metrics.keys(), key=lambda x: int(x.split('@')[1])):
                    f.write(f"  {k:8}: {ndcg_metrics[k]:.4f}\n")
                f.write("\n")
            
            if other_metrics:
                f.write("å…¶ä»–æŒ‡æ ‡:\n")
                for k, v in other_metrics.items():
                    f.write(f"  {k:8}: {v:.4f}\n")
            
            f.write("-" * 40 + "\n\n")
            
            f.write("âš™ï¸ é…ç½®æ‘˜è¦:\n")
            f.write("-" * 40 + "\n")
            
            f.write("æ•°æ®é›†:\n")
            stats = self.config['stats']
            for key, value in stats.items():
                f.write(f"  {key}: {value}\n")
            
            f.write("\næ¨¡å‹æ¶æ„:\n")
            model_config = self.config['model']
            for key, value in model_config.items():
                if key != 'description':
                    f.write(f"  {key}: {value}\n")
            f.write(f"  æè¿°: {model_config['description']}\n")
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    def run(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("\n" + "="*60)
        print("ğŸš€ ACKRec æ–°æ•°æ®é›†è®­ç»ƒæµç¨‹")
        print("="*60)
        
        try:
            # 1. åŠ è½½æ•°æ®
            print("\n1. åŠ è½½æ•°æ®...")
            (self.rating, self.features_item, self.features_user, 
             self.support_user, self.support_item, self.negative) = self.load_data()
            
            # 2. åˆ›å»ºæ¨¡å‹
            print("\n2. åˆ›å»ºæ¨¡å‹...")
            self.create_model()
            
            # 3. å‡†å¤‡æ•°æ®
            print("\n3. å‡†å¤‡æ•°æ®...")
            self.prepare_batch_data()
            
            # 4. è®­ç»ƒæ¨¡å‹
            print("\n4. è®­ç»ƒæ¨¡å‹...")
            final_metrics = self.train()
            
            # 5. æ˜¾ç¤ºç»“æœ
            print("\n" + "="*60)
            print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
            print("="*60)
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            print(f"ğŸ“Š æœ€ç»ˆæŒ‡æ ‡:")
            print_metrics(final_metrics, prefix="  ")
            
            return final_metrics
            
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒæµç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    print("ACKRec æ–°æ•°æ®é›†è®­ç»ƒå™¨")
    print("="*60)
    
    # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ•°æ®ç›®å½•
    data_dir = './processed_data'
    if not os.path.exists(data_dir):
        print(f"âš ï¸ å¤„ç†åçš„æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬:")
        print("  python scripts/create_test_data.py  # åˆ›å»ºæµ‹è¯•æ•°æ®")
        print("  æˆ–")
        print("  python scripts/prepare_dataset.py   # å‡†å¤‡æ‚¨çš„æ•°æ®")
        return 1
    
    # ç¬¬äºŒæ­¥ï¼šå°è¯•åŠ è½½æ•°æ®è·å–ç»Ÿè®¡ä¿¡æ¯
    print("æ­£åœ¨æ£€æŸ¥æ•°æ®é›†...")
    try:
        rating, _, _, _, _, _ = load_new_dataset(data_dir=data_dir)
        dataset_stats = {
            'num_users': rating.shape[0],
            'num_items': rating.shape[1],
            'rating_shape': rating.shape,
            'density': (rating != 0).sum().item() / (rating.shape[0] * rating.shape[1])
        }
        
        print(f"âœ… æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   ç”¨æˆ·æ•°é‡: {dataset_stats['num_users']}")
        print(f"   ç‰©å“æ•°é‡: {dataset_stats['num_items']}")
        print(f"   è¯„åˆ†çŸ©é˜µ: {dataset_stats['rating_shape']}")
        print(f"   äº¤äº’å¯†åº¦: {dataset_stats['density']:.4f}")
        
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½æ•°æ®é›†: {e}")
        print("ä½¿ç”¨é»˜è®¤æ•°æ®é›†ç»Ÿè®¡...")
        dataset_stats = {
            'num_users': 100,
            'num_items': 50,
            'rating_shape': (100, 50),
            'density': 0.1
        }
    
    # ç¬¬ä¸‰æ­¥ï¼šæ ¹æ®æ•°æ®å¤§å°è‡ªåŠ¨é…ç½®
    print("\næ ¹æ®æ•°æ®é›†å¤§å°è‡ªåŠ¨é…ç½®æ¨¡å‹...")
    full_config = ExperimentConfig.setup_experiment(dataset_stats)
    
    # æ˜¾ç¤ºé…ç½®
    model_config = full_config['model']
    print(f"âœ… è‡ªåŠ¨é…ç½®å®Œæˆ:")
    print(f"   æ¨¡å‹ç±»å‹: {model_config['description']}")
    print(f"   éšè—å±‚: {model_config['hidden_dims']}")
    print(f"   è®­ç»ƒè½®æ•°: {model_config['epochs']}")
    print(f"   å­¦ä¹ ç‡: {model_config['learning_rate']}")
    
    # ç¬¬å››æ­¥ï¼šåˆ›å»ºè®­ç»ƒå™¨å¹¶è¿è¡Œ
    print("\n" + "="*60)
    trainer = NewDatasetTrainer(full_config)
    final_metrics = trainer.run()
    
    if final_metrics:
        print("\n" + "="*60)
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        print("="*60)
        print("\nä¸‹ä¸€æ­¥:")
        print("1. æŸ¥çœ‹è®­ç»ƒç»“æœ: analyze_results.py")
        print("2. å¯åŠ¨Webç•Œé¢: streamlit run app.py")
        print("3. ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨è")
        return 0
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main())